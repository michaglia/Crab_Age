---
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
library(tidyverse)
library(reshape2)
```

```{r}
ds1=read.csv("https://raw.githubusercontent.com/guber25/Crab_Age/main/Dataset/Crab_age.csv")
#https://www.kaggle.com/datasets/sidhus/crab-age-prediction
or_dim=dim(ds1)[1]
ds1[,1] <- as.factor(ds1[,1])
```

```{r}
Sex_M=ifelse(ds1$Sex=="M",yes = 1, 0)
Sex_F=ifelse(ds1$Sex=="F",yes = 1, 0)
Sex_I=ifelse(ds1$Sex=="I",yes = 1, 0)
ds1$Sex_M=Sex_M
ds1$Sex_F=Sex_F
ds1$Sex_I=Sex_I
```

```{r}
View(ds1)
```

```{r}
#We try to find some correlation between the variables. Clearly some correlation is present but there are many outliers.
plot(ds1)
```

### WHAT TO DO:

-   exploratory analysis: bella zi dai lo sapete, boxplots, histograms

-   linear regression -\> significativitÃ  delle variabili, collinearity

-   classification -\> regressione logistica e altri modelli che possono
    andare bene, fare alcuni test -\>confusion matrix

-   Unsupervised: k-means clustering ? :-D

```{r}
#In this way we look at the distribution of the variables and we see, again, the presence of outliers and of skewed distribution
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free') 
```

```{r}
glimpse(ds1)
```

```{r}
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_boxplot() + 
    facet_wrap(~key, scales = 'free')
```

```{r}
#we look if there are NULL values or NA values
cat("NULL values count:",sum(is.null(ds1)),"\nNA values count:", sum(is.na(ds1)))
```

#### Outlier removal

```{r}
# OUTLIER REMOVAL (substituting outliers with NA)
for (i in 1:3) {
for (x in ds1 %>% select(-"Sex") %>% names())
{
  value = ds1[,x][ds1[,x] %in% boxplot.stats(ds1[,x])$out]
  ds1[,x][ds1[,x] %in% value] = NA
  ds1 = drop_na(ds1)
} }
head(ds1)
```

```{r}
#Check if there are no more NA values
as.data.frame(colSums(is.na(ds1)))
```

```{r}
#as the plots show, there are no more outliers
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_boxplot() + 
    facet_wrap(~key, scales = 'free')
```

```{r}
cat(((or_dim-dim(ds1)[1])/or_dim)*100,"%")
```

Due to the removal of outliers, we have lost 9.86% of the data.

```{r}
# We see how outlier removal impacted distributions
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free')
```

```{r}
#Now correlations are much more stable
plot(ds1)
```

```{r}
#Correlation matrix
ds1 %>% select(-Sex) %>% cor() %>% melt() %>% #here I am creating the correlation matrix between each other variable (excluding Sex) and "melting" it to creating a data frame which will be used to then create the ggplot 
  ggplot(aes(x=Var2, y=Var1, fill=value)) + 
    geom_tile() +
    geom_text(aes(Var1, Var2, label = round(value, 2)), size = 3, color="white") +
      scale_fill_gradient2(low = "12123220", high = "darkblue",
                         limit = c(-1,1), name="Correlation") +
    theme(axis.text.x = element_text(angle = 45, vjust = .5), 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.background = element_blank())

```

# ^~[Explanatory\ Data\ Analysis]{.smallcaps}~^

-   upload all the libraries

```{r}
library(stats) 
library(ggplot2) 
library(plyr) 
library(dplyr) 
library (cluster) 
install.packages("corrplot") 
library(corrplot) 
install.packages("NbClust") 
library(NbClust) install.packages("factoextra") 
library(factoextra)
```

-   description of the dataset

    ```{r}
    dim(ds1) 
    ds3 <- ds1 
    ds3$Sex <- NULL 
    ds3$Sex_M <- NULL 
    str(ds3) 
    summary(ds3) 
    summary(is.na(ds3)) # there are no missing values
    ```

-   visualizing the data

    ```{r}
    ggplot(ds3, aes(Age, Weight)) + geom_point(alpha = 0.25) + xlab("Age") + ylab("Weight") ds3[, c("Weight", "Age")] = scale(ds3[, c("Age", "Weight")])
    ```

-   correlation matrix

    ```{r}
    cor.result <- cor(ds3) corrplot(cor.result, method="ellipse") +title(main = "Correlation")
    ```

## we can see from here that most of the variables are almost correlated with all of them

# K-means Clustering

-   we need to scale our data now

    ```{r}
    df <- as.data.frame(scale(ds3)) 
    summary(df)
    ```

-   Let's look for the optimal number of clusters

#### Elbow method

```{r}
set.seed(102) 
fviz_nbclust(df, kmeans, method = "wss") + labs(subtitle = "Elbow method") 
# we see that the optimal is 2
```

### Silhouette method

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")+ labs(subtitle = "Silhouette method")
# also here we see that he optimal is 2
```

### Gap statistic

nboot = 50 to keep the function speedy

recommended value: nboot= 500 for your analysis

Use verbose = FALSE to hide computing progression

```{r}
set.seed(123) 
fviz_nbclust(df, kmeans, nstart = 25, method = "gap_stat", nboot = 50)+ labs(subtitle = "Gap statistic method")
# here the optimal is 10
```

2 clusters is selected based on the tests (2 out of 3 tests) above.
Next, we visualise the clusters with triangle mark as the centroid.

```{r}
set.seed(111)
kmean2.simple <- kmeans(df,centers=2, iter.max = 25, nstart=100)
df$cluster <- factor(kmean2.simple$cluster)
summary(df)
```

```{r}
ggplot(data=df, aes(x=Length, y=Age, colour=cluster))+geom_point()+geom_point(data=as.data.frame(kmean2.simple$centers), color ="black", size=4, shape =17)
```

# Evaluation and Interpretation

1.  Internal cluster validation: we need to evaluate the goodness of the
    clustering structure without reference to external information

    ```{r}

    ```

2.  
